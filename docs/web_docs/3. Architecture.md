# Dexbotic System Architecture

This document provides a comprehensive overview of the Dexbotic framework architecture, including the overall system design, training pipeline, and inference service. Dexbotic is designed for training and serving vision language action models (VLAs) for robotic control tasks.

## Overall Framework Diagram

Dexbotic implements a modular architecture that separates data handling, model implementation, and experiment management into distinct layers. The framework is organized into three core layers that work together to provide a complete solution for training and serving vision language action models:

- **Data Layer**: Handles data sources in Dexdata format and provides data processing pipelines for multimodal inputs
- **Model Layer**: Contains the base VLM model and specific implementations (CogAct, OFT)
- **Experiment Layer**: Manages training pipelines and inference services for different model types

This design enables flexible model development, easy experimentation, and scalable deployment for robotic applications.

<div align="center">

```mermaid
graph TB
    %% Data Layer
    subgraph "Data"
        DataSources[Dexdata Format Data Sources]
        DataProcessing[Data Processing Pipeline]
    end

    %% Model Layer
    subgraph "Model"
        BaseModel[DexboticVLM Model]
        CogActModel[CogAct Model]
        OFTModel[OFT Model]
        DiscreteModel[Discrete Model]
        UserModel[Custom Model]
    end

    %% Experiment
    subgraph "Experiment Layer"
        BaseExp[Base Experiment]
        CogActExp[CogAct Experiment]
        OFTExp[OFT Experiment]
        DiscreteExp[Discrete Experiment]
        Training[Training Pipeline]
        Inference[Inference Service]
    end

    %% Connections
    DataSources --> DataProcessing
    DataProcessing --> BaseModel
    
    BaseModel --> CogActModel
    BaseModel --> OFTModel
    BaseModel --> DiscreteModel
    BaseModel --> UserModel
    
    CogActModel --> CogActExp
    OFTModel --> OFTExp
    DiscreteModel --> DiscreteExp
    BaseModel --> BaseExp
    
    BaseExp --> Training
    CogActExp --> Training
    DiscreteExp --> Training
    OFTExp --> Training
    
    Training --> Inference
    
    %% Styles
    classDef dataLayer fill:#f8f9fa,stroke:#6c757d,stroke-width:2px,color:#000
    classDef modelLayer fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    classDef expLayer fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    classDef default fill:#ffffff,stroke:#333333,stroke-width:2px,color:#000
    
    class DataSources,DataProcessing dataLayer
    class BaseModel,CogActModel,OFTModel,DiscreteModel,UserModel modelLayer
    class BaseExp,CogActExp,OFTExp,DiscreteExp,Training,Inference expLayer
```

</div>

## Training Pipeline

The training pipeline illustrates the complete data flow from input to supervision during model training:

- **Data Input**: Multimodal inputs including images, text instructions, and robot state data
- **Data Preprocessing**: Image processing, text tokenization, and action normalization/transformation
- **Model Processing**: Vision encoding, text encoding, multimodal fusion, LLM processing, and action generation
- **Supervision**: Continuous action loss calculation for training supervision


<div align="center">

```mermaid
graph TD
    subgraph "Data Input"
        Images[Image Data]
        Text[Text Data]
        States[Robot State Data]
    end
    
    subgraph "Data Preprocessing"
        ProcessImages[Image Processing]
        ProcessText[Text Tokenization]
        ProcessActions[Action Processing]
    end
    
    subgraph "Model Processing"
        VisionEncoder[Vision Encoder]
        TextEncoder[Text Encoder]
        MultimodalFusion[Multimodal Fusion]
        LLM[LLM]
        ActionGeneration[Action Generation]
    end
    
    subgraph "Supervision"
        Actions[Continuous Action Loss]
    end
    
    Images --> ProcessImages
    Text --> ProcessText
    States --> ProcessActions
    
    ProcessImages --> VisionEncoder
    ProcessText --> TextEncoder
    ProcessActions --> Actions
    
    VisionEncoder --> MultimodalFusion
    TextEncoder --> MultimodalFusion
    MultimodalFusion --> LLM
    LLM --> ActionGeneration
    
    ActionGeneration --> Actions
    
    %% Styles
    classDef inputLayer fill:#f8f9fa,stroke:#6c757d,stroke-width:2px,color:#000
    classDef processLayer fill:#e8f5e8,stroke:#4caf50,stroke-width:2px,color:#000
    classDef modelLayer fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    classDef outputLayer fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    classDef default fill:#ffffff,stroke:#333333,stroke-width:2px,color:#000
    
    class Images,Text,States inputLayer
    class ProcessImages,ProcessText,ProcessActions processLayer
    class VisionEncoder,TextEncoder,MultimodalFusion,LLM,ActionGeneration modelLayer
    class Actions outputLayer
```

</div>

## Inference Service

The inference service provides a streamlined pipeline for action generation during deployment:

- **Client**: DexClient Python client that sends requests with images and text
- **Web API**: Flask-based service that handles HTTP requests and responses
- **Data Processing**: Processes incoming images and text data for model input
- **Model Inference**: VLA (Vision-Language-Action) model that generates continuous actions
- **Action Output**: Returns continuous action commands to the client


<div align="center">

```mermaid
graph TD
    Client[Client<br/>DexClient]
    WebAPI[Web API<br/>Flask Service]
    DataProcessing[Data Processing<br/>Image + Text]
    ModelInference[Model Inference<br/>VLA]
    ActionOutput[Action Output<br/>Continuous Actions]
    
    Client --> WebAPI
    WebAPI --> DataProcessing
    DataProcessing --> ModelInference
    ModelInference --> ActionOutput
    ActionOutput --> WebAPI
    WebAPI --> Client
    
    %% Styles
    classDef clientLayer fill:#f8f9fa,stroke:#6c757d,stroke-width:2px,color:#000
    classDef apiLayer fill:#e8f5e8,stroke:#4caf50,stroke-width:2px,color:#000
    classDef processLayer fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    classDef modelLayer fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    classDef outputLayer fill:#fce4ec,stroke:#e91e63,stroke-width:2px,color:#000
    classDef default fill:#ffffff,stroke:#333333,stroke-width:2px,color:#000
    
    class Client clientLayer
    class WebAPI apiLayer
    class DataProcessing processLayer
    class ModelInference modelLayer
    class ActionOutput outputLayer
```

</div>
